{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B: Meta-Learning Perfomance Prediction\n",
    "\n",
    "In this task, you will use information on training parameters and metadata on multiple OpenML dataset to train a performance predictor that performs well even for unseen datasets. You are provided with config parameters and metafeatures for six datasets. The datasets are split into training datasets and test datasets and you should only train on the training datasets.\n",
    "\n",
    "For questions, you can contact zimmerl@informatik.uni-freiburg.\n",
    "\n",
    "__Note: Please use the dataloading and splits you are provided with in this notebook.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifications:\n",
    "\n",
    "* Data: six_datasets_lw.json\n",
    "* Number of datasets: 6\n",
    "* Training datasets: higgs, vehicle, adult, volkert\n",
    "* Test datasets: Fashion-MNIST, jasmine\n",
    "* Number of configurations: 2000\n",
    "* Available data: architecture parameters and hyperparameters, metafeatures \n",
    "* Target: final validation accuracy\n",
    "* Evaluation metric: MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and splitting data\n",
    "\n",
    "Note: There are 51 steps logged, 50 epochs plus the 0th epoch, prior to any weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd ..\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from api import Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading data...\n",
      "==> No cached data found or cache set to False.\n",
      "==> Reading json data...\n",
      "==> Done.\n"
     ]
    }
   ],
   "source": [
    "bench_dir = \"data/six_datasets_lw.json\"\n",
    "bench = Benchmark(bench_dir, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/metafeatures.json\", \"r\") as f:\n",
    "    metafeatures = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fashion-MNIST', 'adult', 'higgs', 'jasmine', 'vehicle', 'volkert']\n"
     ]
    }
   ],
   "source": [
    "# Dataset split\n",
    "dataset_names = bench.get_dataset_names()\n",
    "print(dataset_names)\n",
    "\n",
    "train_datasets = ['adult', 'higgs', 'vehicle', 'volkert']\n",
    "test_datasets = ['Fashion-MNIST', 'jasmine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (6000,)\n",
      "X_test: (4000,)\n",
      "X_val: (2000,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "def read_data(datasets):\n",
    "    n_configs = bench.get_number_of_configs(datasets[0])\n",
    "    data = [bench.query(dataset_name=d, tag=\"Train/val_accuracy\", config_id=ind) for d in datasets for ind in range(n_configs)]\n",
    "    configs = [bench.query(dataset_name=d, tag=\"config\", config_id=ind) for d in datasets for ind in range(n_configs)]\n",
    "    dataset_names = [d for d in datasets for ind in range(n_configs)]\n",
    "    \n",
    "    y = np.array([curve[-1] for curve in data])\n",
    "    return np.array(configs), y, np.array(dataset_names)\n",
    "\n",
    "class TrainValSplitter():\n",
    "    \"\"\"Splits 25 % data as a validation split.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_names):\n",
    "        self.ind_train, self.ind_val = train_test_split(np.arange(len(X)), test_size=0.25, stratify=dataset_names)\n",
    "        \n",
    "    def split(self, a):\n",
    "        return a[self.ind_train], a[self.ind_val]\n",
    "\n",
    "X, y, dataset_names = read_data(train_datasets)\n",
    "X_test, y_test, dataset_names_test = read_data(test_datasets)\n",
    "\n",
    "tv_splitter = TrainValSplitter(dataset_names=dataset_names)\n",
    "\n",
    "X_train, X_val = tv_splitter.split(X)\n",
    "y_train, y_val = tv_splitter.split(y)\n",
    "dataset_names_train, dataset_names_val = tv_splitter.split(dataset_names)\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"X_val:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains the configuration.\n",
    "\n",
    "__Note__: Not all parameters vary across different configurations. The varying parameters are batch_size, max_dropout, max_units, num_layers, learning_rate, momentum, weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config example:\n",
      "{'batch_size': 18, 'imputation_strategy': 'mean', 'learning_rate_scheduler': 'cosine_annealing', 'loss': 'cross_entropy_weighted', 'network': 'shapedmlpnet', 'max_dropout': 0.49544085228434, 'normalization_strategy': 'standardize', 'optimizer': 'sgd', 'cosine_annealing_T_max': 50, 'cosine_annealing_eta_min': 1e-08, 'activation': 'relu', 'max_units': 194, 'mlp_shape': 'funnel', 'num_layers': 3, 'learning_rate': 0.00022790650718781426, 'momentum': 0.10405150388775633, 'weight_decay': 0.07421575634467908}\n",
      "\n",
      "Meta-feature 'Number of classes': 2.0\n"
     ]
    }
   ],
   "source": [
    "# Take a look at one datapoint\n",
    "datapoint_id = 1\n",
    "config = X_train[datapoint_id]\n",
    "dataset_name = dataset_names_train[datapoint_id]\n",
    "example_metafeature = metafeatures[dataset_name][\"NumberOfClasses\"]\n",
    "\n",
    "print(\"Config example:\", X_train[0], sep=\"\\n\")\n",
    "print(\"\\nMeta-feature 'Number of classes':\", example_metafeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoCorrelation 0.634425994553756\n",
      "ClassEntropy 0.7938438393644257\n",
      "Dimensionality 0.00030711273084640267\n",
      "EquivalentNumberOfAtts 11.068507517484338\n",
      "MajorityClassPercentage 76.07182343065395\n"
     ]
    }
   ],
   "source": [
    "# Look at some metafeatures\n",
    "iterator = iter(metafeatures[dataset_name].items())\n",
    "for ind in range(5):\n",
    "    feature, value = iterator.__next__()\n",
    "    print(feature, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config example:\n",
      "{'batch_size': 18, 'imputation_strategy': 'mean', 'learning_rate_scheduler': 'cosine_annealing', 'loss': 'cross_entropy_weighted', 'network': 'shapedmlpnet', 'max_dropout': 0.49544085228434, 'normalization_strategy': 'standardize', 'optimizer': 'sgd', 'cosine_annealing_T_max': 50, 'cosine_annealing_eta_min': 1e-08, 'activation': 'relu', 'max_units': 194, 'mlp_shape': 'funnel', 'num_layers': 3, 'learning_rate': 0.00022790650718781426, 'momentum': 0.10405150388775633, 'weight_decay': 0.07421575634467908}\n",
      "\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "# Take a look at one datapoint\n",
    "datapoint_id = 23\n",
    "config = X_train[datapoint_id]\n",
    "dataset_name = dataset_names_train[datapoint_id]\n",
    "example_metafeature = metafeatures[dataset_name][\"NumberOfClasses\"]\n",
    "\n",
    "print(\"Config example:\", X_train[0], sep=\"\\n\")\n",
    "#print(\"\\nMeta-feature 'Number of classes':\", example_metafeature)\n",
    "print()\n",
    "print(len(metafeatures[dataset_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to function call (<ipython-input-53-e167ed45bbf9>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-53-e167ed45bbf9>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    metafeatures[dataset_name].item() = metafeatures[dataset_name].item().pop()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m can't assign to function call\n"
     ]
    }
   ],
   "source": [
    "for datapoint_id in range(len(X_train)):\n",
    "    config = X_train[datapoint_id]\n",
    "    dataset_name = dataset_names_train[datapoint_id].item()\n",
    "    if dataset_name == 'vehicle':\n",
    "        for _ in range(len(metafeatures['higgs'])):\n",
    "            print(type(metafeatures[dataset_name]))\n",
    "            metafeatures[dataset_name].item() = metafeatures[dataset_name].item().pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "{'adult': [0.634425994553756, 0.7938438393644257, 0.00030711273084640267, 11.068507517484338, 76.07182343065395, 37155.0, 3.44192266924963, 152.69309629815925, 189664.13459727284, 0.16542318099233, 41.0, 11.89465899659272, 105604.02542315713, 1.7809891200338273, 30.359637681213712, 31819.974765570616, 0.0717209468494675, 23.832203118732952, 11.222222222222221, 3.063860808334838, 18914.620326608216, 0.795215031650176, -0.18426874062378573, 10.078088530363212, 0.00818704228545, 2.0, -0.31652485666094055, 2.5709727555918307, 23.928176569346054, 11687.0, 2.0, 2.0, 15.0, 48842.0, 3620.0, 6465.0, 6.0, 9.0, 13.333333333333334, 7.411653904426519, 0.8824372466319971, 40.0, 60.0, 0.8343198263526672, 0.42324176943424885, 31.502211211662093, 0.01068033823523, 0.09993102873665205, 9.936326207089905, 1.6008102873649352, 4.504453651154136, 63.96234797919819, 0.0623734340938, 0.9982360975676194, 208.3575310294492, 2.736773252802121, 53.18403354052852, 48225.334368985714, 0.14076964990508, 6.4010213924528, 31990.020649029368, 12.152960316089429], 'higgs': [0.5012085793837775, 0.9976423339342788, 0.00029576746557878636, nan, 52.85772565017848, 51827.0, nan, 63.889738287879354, 1.0501287597024396, nan, 2.0, 5.981690289185084, 1.3968091904244635, nan, 7.897868562324222, 0.6073695187399869, nan, nan, 2.0, 1.3574091204635736, 0.7734372841717916, nan, -1.8618527812710703, -0.007896418321767898, nan, 2.0, -0.005582304821509899, 0.16456533466462578, 47.14227434982152, 46223.0, 1.0, 2.0, 29.0, 98050.0, 1.0, 9.0, 28.0, 1.0, 3.4482758620689653, 0.0010198878123406426, 0.0003165169072781304, 96.55172413793103, 3.4482758620689653, nan, -1.2011141288244025, -0.00024107026685333605, nan, 0.0016372995206194716, 0.49075678522081984, nan, -0.026568040146459415, 0.9864974774019043, nan, 0.5998566551326563, 1.0047700404855044, nan, 11.233955227411903, 1.0026634234089684, nan, 2.2951169764631905, 1.0085998580390358, 0.0], 'volkert': [0.14778164605806993, 2.961279665138157, 0.003104098782370091, nan, 21.961927628194136, 12806.0, nan, 1338.965301161398, 1.603557810444178, nan, 10.0, 28.83417325173541, 0.8509326916247117, nan, 47.91428742977393, 0.12905913877058975, nan, nan, 10.0, 3.1738964926735753, 0.07076387610160444, nan, -1.2346500080758578, 0.0, nan, 10.0, -1.3885132862025662, 0.0, 2.3340764877379523, 1361.0, 0.0, 10.0, 181.0, 58310.0, 0.0, 0.0, 180.0, 1.0, 0.0, 0.0, 0.0, 99.4475138121547, 0.5524861878453038, nan, 1.2802116136926767, 0.0048930424192677015, nan, 0.3673368685681075, 0.003703367915541548, nan, 3.587601549615851, 0.012837357632224405, nan, 1.1450933717436125, 0.006976476940008107, nan, 19.118820363694873, 0.039962534707554453, nan, 3.912033503483554, 0.08444881034275072, 0.0]}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-dimensional arrays cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-c76458fecc70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmetadata_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetadata_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
     ]
    }
   ],
   "source": [
    "metadata_new = {}\n",
    "train_datasets = ['adult', 'higgs', 'volkert']\n",
    "for i in enumerate(train_datasets):\n",
    "    data = []\n",
    "    print(type(metafeatures[i[1]]))\n",
    "    for k, v in metafeatures[i[1]].items():\n",
    "        data.append(v)\n",
    "    metadata_new[i[1]] = data\n",
    "print(metadata_new)\n",
    "np.concatenate((data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantPerformancePredictor():\n",
    "    \"\"\"A predictor that predicts the mean of the performances seen on the training data.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.constant_prediction = 0\n",
    "        \n",
    "    def fit(self, X, y, dataset_names, metafeatures):\n",
    "        self.constant_prediction = np.mean(y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = [self.constant_prediction] * len(X)\n",
    "        return predictions\n",
    "    \n",
    "def score(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283.5325483378039\n"
     ]
    }
   ],
   "source": [
    "# Train and validate\n",
    "predictor = ConstantPerformancePredictor()\n",
    "predictor.fit(X_train, y_train, dataset_names_train, metafeatures)\n",
    "preds = predictor.predict(X_val)\n",
    "mse = score(y_val, preds)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test score: 477.5415315131301\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation\n",
    "final_preds = predictor.predict(X_test)\n",
    "final_score = score(y_test, final_preds)\n",
    "print(\"Final test score:\", final_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
